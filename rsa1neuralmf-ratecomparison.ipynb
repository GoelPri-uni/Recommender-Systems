{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11060274,"sourceType":"datasetVersion","datasetId":6891228,"isSourceIdPinned":false},{"sourceId":11107019,"sourceType":"datasetVersion","datasetId":6924481}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"RS A1","provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"143d93c8184749bcb07d15e362f0853f":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6f613011684042e78b529f57accf51ee"],"layout":"IPY_MODEL_3b29ff3472d9489887d1fa6c8aaaf689"}},"ac8dddf62be7438b96d5c2c845cec012":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2f818f00c6b457ea486921eda5d0991","placeholder":"​","style":"IPY_MODEL_6ce26d6b4a8d40b1a2de08a0c0ca99b3","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"2f5df0f741f24be9a3b24844bcf3c0ec":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_dd30bd5fa1c44e728f97b7d96f8727f5","placeholder":"​","style":"IPY_MODEL_c60a33cf39004bf0b1d89b7b42c637c8","value":"jiaxuany"}},"031658ea709a45549a8cd8361f2a3f9b":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_54d50713c19748e29231066f349dcaf2","placeholder":"​","style":"IPY_MODEL_312aff784d0042598203a8377b98b995","value":""}},"c50d059026274b0b87374a3905667af7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_fad348e4fa5a4e1a82af3f4e7a50607d","style":"IPY_MODEL_973f6edd67dd4b6d80fc7dc1d431e807","tooltip":""}},"6ec26bab9dbf4dcc837dfaa3b30b5351":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_228ea9f6b14a4da4b2ac960adcd05c04","placeholder":"​","style":"IPY_MODEL_13e6c7f2fb3549fabc82ab0f5e293c8b","value":"\n<b>Thank You</b></center>"}},"3b29ff3472d9489887d1fa6c8aaaf689":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"e2f818f00c6b457ea486921eda5d0991":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ce26d6b4a8d40b1a2de08a0c0ca99b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd30bd5fa1c44e728f97b7d96f8727f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60a33cf39004bf0b1d89b7b42c637c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54d50713c19748e29231066f349dcaf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"312aff784d0042598203a8377b98b995":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fad348e4fa5a4e1a82af3f4e7a50607d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"973f6edd67dd4b6d80fc7dc1d431e807":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"228ea9f6b14a4da4b2ac960adcd05c04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e6c7f2fb3549fabc82ab0f5e293c8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c251c2346a84d8b8b806d136ffa7188":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced99575a96b479581efa61b8df2b0f2","placeholder":"​","style":"IPY_MODEL_4a407c9ed4a14462bca62e95fba89851","value":"Connecting..."}},"ced99575a96b479581efa61b8df2b0f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a407c9ed4a14462bca62e95fba89851":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f613011684042e78b529f57accf51ee":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e855ff168ed4dd693702bcb731ab239","placeholder":"​","style":"IPY_MODEL_4ac724936f6f4fada4a78bdcdf86277a","value":"Kaggle credentials successfully validated."}},"1e855ff168ed4dd693702bcb731ab239":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ac724936f6f4fada4a78bdcdf86277a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nmp.set_start_method('spawn', force=True)\n# import data as data\n# import model_evaluation as evaluation\nimport torch.optim as optim\nimport torch._dynamo\nimport numpy as np\nimport heapq\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ntorch._dynamo.config.suppress_errors = True\n\nrandom.seed(1000)\n\ndef load_data(filename, threshold=4, train_ratio=0.7, test_ratio=0.15):\n    \"\"\"\n    Load dataset and split data on a per-user basis.\n\n    Args:\n        filename (str): Path to the ratings file.\n        train_ratio (float): Percentage of interactions used for training.\n        test_ratio (float): Percentage of interactions used for testing.\n\n    Returns:\n        train_dict, val_dict, test_dict, movie_num, user_num\n    \"\"\"\n    user_ratings = {}  # Store each user's interactions (user_id: [(movie_id, label), ...])\n    movie_num = -1\n    user_num = -1\n\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            user_id, movie_id, rating, _ = map(int, line.strip().split(\"::\"))\n            \n            # Ignore rating 3\n            if rating == 3:\n                continue\n            \n            # Map ratings >=4 to 1, ratings 1 or 2 to 0\n            label = 1 if rating >= threshold else 0\n\n            if user_id not in user_ratings:\n                user_ratings[user_id] = []\n            user_ratings[user_id].append((movie_id, label))\n\n            # movie and user number\n            movie_num = max(movie_num, movie_id)\n            user_num = max(user_num, user_id)\n\n    train_dict, val_dict, test_dict = {}, {}, {}\n   \n    ######### divide by users? cold start #######\n    # Divide each user's movie interactions by proportion\n    for user_id, interactions in user_ratings.items():\n        random.shuffle(interactions)  # shuffle\n\n        total_interactions = len(interactions)\n        train_end = int(train_ratio * total_interactions)\n        val_end = int((train_ratio + test_ratio) * total_interactions)\n\n        train_dict[user_id] = interactions[:train_end]\n        val_dict[user_id] = interactions[train_end:val_end]\n        test_dict[user_id] = interactions[val_end:]\n\n    return train_dict, val_dict, test_dict, movie_num, user_num","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"di74PRcz9nmA","outputId":"f196872a-3e4e-40df-acc2-3d1e7e2bd3e1","execution":{"iopub.status.busy":"2025-03-25T08:04:10.522877Z","iopub.execute_input":"2025-03-25T08:04:10.523342Z","iopub.status.idle":"2025-03-25T08:04:10.531035Z","shell.execute_reply.started":"2025-03-25T08:04:10.523306Z","shell.execute_reply":"2025-03-25T08:04:10.530066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data_rate(filename, threshold=4, train_ratio=0.7, test_ratio=0.15):\n    \"\"\"\n    Load dataset and split data on a per-user basis.\n\n    Args:\n        filename (str): Path to the ratings file.\n        train_ratio (float): Percentage of interactions used for training.\n        test_ratio (float): Percentage of interactions used for testing.\n\n    Returns:\n        train_dict, val_dict, test_dict, movie_num, user_num\n    \"\"\"\n    user_ratings = {}  # Store each user's interactions (user_id: [(movie_id, label), ...])\n    movie_num = -1\n    user_num = -1\n\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            user_id, movie_id, rating, _ = map(int, line.strip().split(\"::\"))\n            \n            # # Ignore rating 3\n            # if rating == 3:\n            #     continue\n            \n            # Map ratings >=4 to 1, ratings 1 or 2 to 0\n            label = 1 if rating >= threshold else 0\n\n            if user_id not in user_ratings:\n                user_ratings[user_id] = []\n            user_ratings[user_id].append((movie_id, label))\n\n            # movie and user number\n            movie_num = max(movie_num, movie_id)\n            user_num = max(user_num, user_id)\n\n    train_dict, val_dict, test_dict = {}, {}, {}\n   \n    ######### divide by users? cold start #######\n    # Divide each user's movie interactions by proportion\n    for user_id, interactions in user_ratings.items():\n        random.shuffle(interactions)  # shuffle\n\n        total_interactions = len(interactions)\n        train_end = int(train_ratio * total_interactions)\n        val_end = int((train_ratio + test_ratio) * total_interactions)\n\n        train_dict[user_id] = interactions[:train_end]\n        val_dict[user_id] = interactions[train_end:val_end]\n        test_dict[user_id] = interactions[val_end:]\n\n    return train_dict, val_dict, test_dict, movie_num, user_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:11.442386Z","iopub.execute_input":"2025-03-25T08:04:11.442704Z","iopub.status.idle":"2025-03-25T08:04:11.449036Z","shell.execute_reply.started":"2025-03-25T08:04:11.442679Z","shell.execute_reply":"2025-03-25T08:04:11.448097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data_meanStd(filename, train_ratio=0.7, test_ratio=0.15):\n    \"\"\"\n    Load dataset and split data on a per-user basis using a dynamic threshold \n    based on user-specific mean (μ_u) and standard deviation (σ_u).\n    \n    Args:\n        filename (str): Path to the ratings file.\n        train_ratio (float): Percentage of interactions used for training.\n        test_ratio (float): Percentage of interactions used for testing.\n\n    Returns:\n        train_dict, val_dict, test_dict, movie_num, user_num\n    \"\"\"\n    user_ratings = {}  # Store each user's interactions (user_id: [(movie_id, rating), ...])\n    movie_num = -1\n    user_num = -1\n\n    # Step 1: Read data and store user ratings\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            user_id, movie_id, rating, _ = map(int, line.strip().split(\"::\"))\n            \n            if user_id not in user_ratings:\n                user_ratings[user_id] = []\n            user_ratings[user_id].append((movie_id, rating))\n\n            # Update max movie and user IDs\n            movie_num = max(movie_num, movie_id)\n            user_num = max(user_num, user_id)\n\n    # Step 2: Compute user-specific mean and standard deviation\n    train_dict, val_dict, test_dict = {}, {}, {}\n\n    for user_id, interactions in user_ratings.items():\n        ratings = np.array([r for _, r in interactions])\n        mu_u = np.mean(ratings)\n        sigma_u = np.std(ratings)\n\n        # Step 3: Convert ratings to binary labels based on user-specific threshold\n        labeled_interactions = []\n        for movie_id, rating in interactions:\n            if rating >= mu_u:\n                label = 1  # Positive\n            elif rating < mu_u - sigma_u:\n                label = 0  # Negative\n            else:\n                continue  # Ignore ratings in the middle range\n            \n            labeled_interactions.append((movie_id, label))\n\n        # Step 4: Shuffle and split into train/val/test sets\n        random.shuffle(labeled_interactions)\n        total_interactions = len(labeled_interactions)\n        train_end = int(train_ratio * total_interactions)\n        val_end = int((train_ratio + test_ratio) * total_interactions)\n\n        train_dict[user_id] = labeled_interactions[:train_end]\n        val_dict[user_id] = labeled_interactions[train_end:val_end]\n        test_dict[user_id] = labeled_interactions[val_end:]\n\n    return train_dict, val_dict, test_dict, movie_num, user_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:15.633669Z","iopub.execute_input":"2025-03-25T08:04:15.634007Z","iopub.status.idle":"2025-03-25T08:04:15.641681Z","shell.execute_reply.started":"2025-03-25T08:04:15.633968Z","shell.execute_reply":"2025-03-25T08:04:15.640828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_input_data_nointeract(train_dict, non_interacted_movies):\n    user_input, movie_input, labels = [], [], []\n\n    for u, rate_list in train_dict.items():\n        # positive samples in train set\n        for movie_id, label in rate_list:\n            user_input.append(u)\n            movie_input.append(movie_id)\n            labels.append(label)\n        \n        # collect all movies not interacted with user\n        non_interacted_items = non_interacted_movies.get(u, [])\n        \n        # Add all non-interacted movies as negative samples with label 0\n        for movie_id in non_interacted_items:\n            user_input.append(u)\n            movie_input.append(movie_id)\n            labels.append(0)\n\n    return user_input, movie_input, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:18.970616Z","iopub.execute_input":"2025-03-25T08:04:18.970922Z","iopub.status.idle":"2025-03-25T08:04:18.975798Z","shell.execute_reply.started":"2025-03-25T08:04:18.9709Z","shell.execute_reply":"2025-03-25T08:04:18.974887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######### rate>=4 negative sample? interaction #######\ndef get_input_data(train_dict, non_interacted_movies, negative_num):\n    user_input, movie_input, labels = [], [], []\n\n    for u, rate_list in train_dict.items():\n        # positive samples in train set\n        for movie_id, label in rate_list:\n            user_input.append(u)\n            movie_input.append(movie_id)\n            labels.append(label)\n        \n        # collect all movies not interacted with user\n        non_interacted_items = non_interacted_movies.get(u, [])\n        # negative samples\n        for _ in range(negative_num):\n            if non_interacted_items:\n                movie_id = random.choice(non_interacted_items)\n                user_input.append(u)\n                movie_input.append(movie_id)\n                labels.append(0)\n\n    return user_input, movie_input, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:22.607324Z","iopub.execute_input":"2025-03-25T08:04:22.607609Z","iopub.status.idle":"2025-03-25T08:04:22.612709Z","shell.execute_reply.started":"2025-03-25T08:04:22.607588Z","shell.execute_reply":"2025-03-25T08:04:22.611726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num):\n    non_interacted_movies = {}\n\n    for u in train_dict:\n        # Get the movies that the user has interacted with (including train, val, test)\n        interacted_movies = set(movie_id for movie_id, _ in train_dict.get(u, []))\n        if u in val_dict:\n            interacted_movies.update(movie_id for movie_id, _ in val_dict.get(u, []))\n        if u in test_dict:\n            interacted_movies.update(movie_id for movie_id, _ in test_dict.get(u, []))\n\n        # Get the movies that the user has not interacted with\n        all_movies = set(range(1, movie_num + 1))\n        non_interacted_movies[u] = list(all_movies - interacted_movies)\n\n    return non_interacted_movies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:24.616239Z","iopub.execute_input":"2025-03-25T08:04:24.61657Z","iopub.status.idle":"2025-03-25T08:04:24.621638Z","shell.execute_reply.started":"2025-03-25T08:04:24.616543Z","shell.execute_reply":"2025-03-25T08:04:24.620859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NeuMF(nn.Module):\n    def __init__(self, num_users, num_items, mf_dim=10, layers=[10], reg_mf=0.0, reg_layers=None):\n        super(NeuMF, self).__init__()\n\n        if reg_layers is None:\n            reg_layers = [0] * len(layers)\n\n        assert len(layers) == len(reg_layers)\n        \n        # GMF Embeddings\n        self.user_embedding_gmf = nn.Embedding(num_users, mf_dim)\n        self.item_embedding_gmf = nn.Embedding(num_items, mf_dim)\n\n        # MLP Embeddings\n        self.user_embedding_mlp = nn.Embedding(num_users, layers[0] // 2)\n        self.item_embedding_mlp = nn.Embedding(num_items, layers[0] // 2)\n\n        # Initialize embedding weights\n        nn.init.normal_(self.user_embedding_gmf.weight, std=0.01)\n        nn.init.normal_(self.item_embedding_gmf.weight, std=0.01)\n        nn.init.normal_(self.user_embedding_mlp.weight, std=0.01)\n        nn.init.normal_(self.item_embedding_mlp.weight, std=0.01)\n\n        # MLP Layers\n        self.mlp_layers = nn.Sequential()\n        input_dim = layers[0]  # Initial input size (concatenated user & item embeddings)\n        for i in range(1, len(layers)):\n            self.mlp_layers.add_module(f\"fc{i}\", nn.Linear(input_dim, layers[i]))\n            self.mlp_layers.add_module(f\"relu{i}\", nn.ReLU())\n            input_dim = layers[i]\n\n        # Output layer: combines GMF and MLP outputs\n        self.fc_output = nn.Linear(mf_dim + layers[-1], 1)  # GMF (mf_dim) + MLP (last layer size)\n\n        # Regularization parameters\n        self.reg_mf = reg_mf\n        self.reg_layers = reg_layers\n\n    def forward(self, user_indices, item_indices):\n        \"\"\" Forward pass for NeuMF model \"\"\"\n\n        # GMF Forward Pass: Element-wise multiplication\n        user_latent_gmf = self.user_embedding_gmf(user_indices)\n        item_latent_gmf = self.item_embedding_gmf(item_indices)\n        gmf_out = torch.mul(user_latent_gmf, item_latent_gmf)  # Element-wise multiplication\n\n        # MLP Forward Pass: Concatenate embeddings and pass through MLP layers\n        user_latent_mlp = self.user_embedding_mlp(user_indices)\n        item_latent_mlp = self.item_embedding_mlp(item_indices)\n        mlp_input = torch.cat((user_latent_mlp, item_latent_mlp), dim=-1)  # Concatenation\n        mlp_out = self.mlp_layers(mlp_input)\n\n        # Combine GMF and MLP outputs\n        combined = torch.cat((gmf_out, mlp_out), dim=-1)\n        prediction = torch.sigmoid(self.fc_output(combined))  # Final prediction\n\n        return prediction\n\n    def get_regularization_loss(self):\n        \"\"\" Compute L2 regularization loss for embeddings and MLP layers \"\"\"\n        reg_loss = 0\n        reg_loss += self.reg_mf * (torch.norm(self.user_embedding_gmf.weight, p=2) + torch.norm(self.item_embedding_gmf.weight, p=2))\n\n        for i, layer in enumerate(self.mlp_layers):\n            if isinstance(layer, nn.Linear):\n                reg_loss += self.reg_layers[i] * torch.norm(layer.weight, p=2)\n\n        return reg_loss","metadata":{"trusted":true,"id":"7scibCrO9nmB","execution":{"iopub.status.busy":"2025-03-25T08:04:26.36673Z","iopub.execute_input":"2025-03-25T08:04:26.367016Z","iopub.status.idle":"2025-03-25T08:04:26.377203Z","shell.execute_reply.started":"2025-03-25T08:04:26.366994Z","shell.execute_reply":"2025-03-25T08:04:26.376228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import heapq\nfrom sklearn.metrics import precision_score, recall_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:29.784332Z","iopub.execute_input":"2025-03-25T08:04:29.784605Z","iopub.status.idle":"2025-03-25T08:04:30.250682Z","shell.execute_reply.started":"2025-03-25T08:04:29.784585Z","shell.execute_reply":"2025-03-25T08:04:30.249926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_evaluation(model, val_dict, device, K=10):\n    model.to(device)\n    model.eval()  \n    user_input = []\n    movie_input = []\n    labels = []\n    \n    for u, interactions in val_dict.items():\n        for movie_id, label in interactions:\n            user_input.append(u)\n            movie_input.append(movie_id)\n            labels.append(label)\n    \n    user_input = torch.tensor(user_input, dtype=torch.long, device=device)\n    movie_input = torch.tensor(movie_input, dtype=torch.long, device=device)\n\n    with torch.no_grad():\n        predictions = model(user_input, movie_input).squeeze(-1).cpu().numpy()  \n\n    predictions_dict = {}\n    for u, m, score in zip(user_input.cpu().tolist(), movie_input.cpu().tolist(), predictions):\n        if u not in predictions_dict:\n            predictions_dict[u] = {}\n        predictions_dict[u][m] = score\n\n    precision_list = []\n    recall_list = []\n    \n    for u, interactions in val_dict.items():\n        pos_movies = {m for m, label in interactions if label == 1}\n        if not pos_movies:\n            continue\n\n        if u not in predictions_dict:\n            continue\n        pred_scores = predictions_dict[u]\n\n        top_k_items = np.array(sorted(pred_scores.keys(), key=lambda x: pred_scores[x], reverse=True))[:K]\n\n        # Calculate Precision@10\n        relevant_in_top_k = sum(1 for movie_id in top_k_items if movie_id in pos_movies)\n        precision_at_10 = relevant_in_top_k / K\n        precision_list.append(precision_at_10)\n\n        # Calculate Recall@10\n        recall_at_10 = relevant_in_top_k / len(pos_movies)\n        recall_list.append(recall_at_10)\n\n    # Calculate average Precision@10 and Recall@10\n    avg_precision_at_10 = np.mean(precision_list) if precision_list else 0\n    avg_recall_at_10 = np.mean(recall_list) if recall_list else 0\n\n    # Calculate F1@10\n    if avg_precision_at_10 + avg_recall_at_10 > 0:\n        f1_at_10 = 2 * (avg_precision_at_10 * avg_recall_at_10) / (avg_precision_at_10 + avg_recall_at_10)\n    else:\n        f1_at_10 = 0\n\n    return avg_precision_at_10, avg_recall_at_10, f1_at_10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:31.102087Z","iopub.execute_input":"2025-03-25T08:04:31.102555Z","iopub.status.idle":"2025-03-25T08:04:31.111628Z","shell.execute_reply.started":"2025-03-25T08:04:31.102525Z","shell.execute_reply":"2025-03-25T08:04:31.110595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def model_evaluation(model, val_dict, device, K=10):\n#     model.to(device)\n#     model.eval()  \n#     user_input = []\n#     movie_input = []\n#     labels = []\n    \n#     for u, interactions in val_dict.items():\n#         for movie_id, label in interactions:\n#             user_input.append(u)\n#             movie_input.append(movie_id)\n#             labels.append(label)\n    \n#     user_input = torch.tensor(user_input, dtype=torch.long, device=device)\n#     movie_input = torch.tensor(movie_input, dtype=torch.long, device=device)\n\n#     with torch.no_grad():\n#         predictions = model(user_input, movie_input).squeeze(-1).cpu().numpy()  \n\n#     predictions_dict = {}\n#     for u, m, score in zip(user_input.cpu().tolist(), movie_input.cpu().tolist(), predictions):\n#         if u not in predictions_dict:\n#             predictions_dict[u] = {}\n#         predictions_dict[u][m] = score\n\n#     recall_list = []\n#     ndcg_list = []\n\n#     for u, interactions in val_dict.items():\n#         pos_movies = {m for m, label in interactions if label == 1}\n#         if not pos_movies:\n#             continue\n\n#         if u not in predictions_dict:\n#             continue\n#         pred_scores = predictions_dict[u]\n\n#         top_k_items = np.array(sorted(pred_scores.keys(), key=lambda x: pred_scores[x], reverse=True))[:K]\n\n#         recall = len(pos_movies.intersection(top_k_items)) / len(pos_movies)\n#         recall_list.append(recall)\n\n#         ndcg = calculate_ndcg(pos_movies, top_k_items, K)\n#         ndcg_list.append(ndcg)\n\n#     avg_recall = np.mean(recall_list) if recall_list else 0\n#     avg_ndcg = np.mean(ndcg_list) if ndcg_list else 0\n\n#     return avg_recall, avg_ndcg\n\n\n# def calculate_ndcg(pos_movies, top_k_items, K):\n#     \"\"\"\n#     Calculate NDCG for the top-K recommended items.\n\n#     Args:\n#     - pos_movies: A set of relevant (ground truth) items for the user.\n#     - top_k_items: A list of the top-K recommended items.\n#     - K: The number of top items considered for evaluation.\n\n#     Returns:\n#     - NDCG score.\n#     \"\"\"\n#     K = min(K, len(top_k_items))  # Adjust K to avoid overestimation\n\n#     # Compute DCG\n#     dcg = sum(1 / np.log2(i + 2) for i, item in enumerate(top_k_items[:K]) if item in pos_movies)\n\n#     # Compute IDCG (Ideal DCG)\n#     ideal_hits = min(K, len(pos_movies))  # Can't be more than positive items\n#     idcg = sum(1 / np.log2(i + 2) for i in range(ideal_hits))\n\n#     return dcg / idcg if idcg > 0 else 0","metadata":{"trusted":true,"id":"SwLXLy0Y9nmB","execution":{"iopub.status.busy":"2025-03-25T08:04:34.789231Z","iopub.execute_input":"2025-03-25T08:04:34.789589Z","iopub.status.idle":"2025-03-25T08:04:34.793722Z","shell.execute_reply.started":"2025-03-25T08:04:34.789561Z","shell.execute_reply":"2025-03-25T08:04:34.792658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:37.57095Z","iopub.execute_input":"2025-03-25T08:04:37.571256Z","iopub.status.idle":"2025-03-25T08:04:37.57483Z","shell.execute_reply.started":"2025-03-25T08:04:37.571223Z","shell.execute_reply":"2025-03-25T08:04:37.573857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\ntrain_recalls = []\ntrain_ndcgs = []\nrecalls = []\nndcgs = []\nf1s = []\npatience = 10\ncounter = 0\nbest_val_loss = float('inf')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_dict, val_dict, test_dict, movie_num, user_num = load_data('/kaggle/input/mmmmmm/ratings.dat')\nnegative_num = 10\nnon_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n\nuser_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n# latent_dim = 8\n\nbatch_size = 256\nnum_epochs = 30\nmodel = NeuMF(user_num+1, movie_num+1, 10, [10, 16]).to(device)\nmodel = torch.compile(model)\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n\ncriterion = nn.BCEWithLogitsLoss()\nscaler = torch.amp.GradScaler('cuda')\n\nuser_input = torch.tensor(user_input, dtype=torch.long).to(device)\nmovie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\nlabels = torch.tensor(labels, dtype=torch.float32).to(device)\n\ndataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n\n# val_user_input = []\n# val_movie_input = []\n# val_labels = []\n\n# for u in val_dict.keys():\n#     for (movie_id, label) in val_dict[u]:\n#         val_user_input.append(u)\n#         val_movie_input.append(movie_id)\n#         val_labels.append(label)\nval_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n\nval_dict = defaultdict(list)\n\nfor user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n    val_dict[user_id].append((movie_id, label))\n\nval_dict = dict(val_dict)\n\nval_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\nval_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\nval_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n\nval_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch_users, batch_items, batch_labels in dataloader:\n        batch_users = batch_users.to(device)\n        batch_items = batch_items.to(device)\n        batch_labels = batch_labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda'):\n            predictions = model(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_users, batch_items, batch_labels in val_dataloader:\n            batch_users = batch_users.to(device)\n            batch_items = batch_items.to(device)\n            batch_labels = batch_labels.to(device)\n            predictions = model(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n            val_loss += loss.item()\n        val_loss_avg = val_loss / len(val_dataloader)\n        scheduler.step(val_loss_avg)\n        print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n    train_losses.append(total_loss / len(dataloader))\n    val_losses.append(val_loss_avg)\n\n    # train_recall, train_ndcg = model_evaluation(model, train_dict, device, K=10)\n    _, _, f1 = model_evaluation(model, val_dict, device, K=10)\n    # train_recalls.append(train_recall)\n    # train_ndcgs.append(train_ndcg)\n    # recalls.append(recall)\n    # ndcgs.append(ndcg)\n    f1s.append(f1)\n\n    # early stop\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        counter = 0  \n        torch.save(model.state_dict(), \"./best_model.pth\") \n    else:\n        counter += 1\n        print(f\"Early Stopping Counter: {counter}/{patience}\")\n        if counter >= patience:\n            print(\"Early stopping triggered! Stopping training.\")\n            break     ","metadata":{"trusted":true,"id":"f2E7-ucF9nmB","outputId":"ac1192d0-7f53-4d1c-d62d-e13dbb077d74","scrolled":true,"execution":{"iopub.status.busy":"2025-03-25T08:04:38.417793Z","iopub.execute_input":"2025-03-25T08:04:38.418116Z","iopub.status.idle":"2025-03-25T09:34:15.245147Z","shell.execute_reply.started":"2025-03-25T08:04:38.418089Z","shell.execute_reply":"2025-03-25T09:34:15.244365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses_meanStd = []\nval_losses_meanStd = []\ntrain_recalls_meanStd = []\ntrain_ndcgs_meanStd = []\nrecalls_meanStd = []\nf1s_meanStd = []\nndcgs_meanStd = []\npatience = 10\ncounter = 0\nbest_val_loss = float('inf')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_dict, val_dict, test_dict, movie_num, user_num = load_data_meanStd('/kaggle/input/mmmmmm/ratings.dat')\nnegative_num = 10\nnon_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n\nuser_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n# latent_dim = 8\n\nbatch_size = 256\nnum_epochs = 30\nmodel_meanStd = NeuMF(user_num+1, movie_num+1,10, [10, 16]).to(device)\nmodel_meanStd = torch.compile(model_meanStd)\noptimizer = optim.Adam(model_meanStd.parameters(), lr=0.0001, weight_decay=1e-5)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n\ncriterion = nn.BCEWithLogitsLoss()\nscaler = torch.amp.GradScaler('cuda')\n\nuser_input = torch.tensor(user_input, dtype=torch.long).to(device)\nmovie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\nlabels = torch.tensor(labels, dtype=torch.float32).to(device)\n\ndataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\n# val_user_input = []\n# val_movie_input = []\n# val_labels = []\n\n# for u in val_dict.keys():\n#     for (movie_id, label) in val_dict[u]:\n#         val_user_input.append(u)\n#         val_movie_input.append(movie_id)\n#         val_labels.append(label)\n\nval_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\nval_dict = defaultdict(list)\n\nfor user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n    val_dict[user_id].append((movie_id, label))\n\nval_dict = dict(val_dict)\n\nval_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\nval_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\nval_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n\nval_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\nfor epoch in range(num_epochs):\n    model_meanStd.train()\n    total_loss = 0\n\n    for batch_users, batch_items, batch_labels in dataloader:\n        batch_users = batch_users.to(device)\n        batch_items = batch_items.to(device)\n        batch_labels = batch_labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda'):\n            predictions = model_meanStd(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n\n    model_meanStd.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_users, batch_items, batch_labels in val_dataloader:\n            batch_users = batch_users.to(device)\n            batch_items = batch_items.to(device)\n            batch_labels = batch_labels.to(device)\n            predictions = model_meanStd(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n            val_loss += loss.item()\n        val_loss_avg = val_loss / len(val_dataloader)\n        scheduler.step(val_loss_avg)\n        print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n    train_losses_meanStd.append(total_loss / len(dataloader))\n    val_losses_meanStd.append(val_loss_avg)\n        \n    _, _, f1 = model_evaluation(model, val_dict, device, K=10)\n    # train_recall, train_ndcg = model_evaluation(model_meanStd, train_dict, device, K=10)\n    # train_recalls_meanStd.append(train_recall)\n    # train_ndcgs_meanStd.append(train_ndcg)\n    f1s_meanStd.append(f1)\n    # early stop\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        counter = 0  \n        torch.save(model_meanStd.state_dict(), \"./best_model_meanStd.pth\") \n    else:\n        counter += 1\n        print(f\"Early Stopping Counter: {counter}/{patience}\")\n        if counter >= patience:\n            print(\"Early stopping triggered! Stopping training.\")\n            break     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:06:33.535964Z","iopub.execute_input":"2025-03-24T21:06:33.536554Z","iopub.status.idle":"2025-03-24T22:05:36.351587Z","shell.execute_reply.started":"2025-03-24T21:06:33.536521Z","shell.execute_reply":"2025-03-24T22:05:36.350737Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4","metadata":{}},{"cell_type":"code","source":"train_losses_rate = []\nval_losses_rate = []\nf1s_rate = []\nrecalls_rate = []\nndcgs_rate = []\ntrain_recalls_rate = []\ntrain_ndcgs_rate = []\npatience = 10\ncounter = 0\nbest_val_loss = float('inf')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/kaggle/input/mmmmmm/ratings.dat')\nnegative_num = 10\nnon_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n\nuser_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n# latent_dim = 8\n\nbatch_size = 256\nnum_epochs = 30\nmodel_4 = NeuMF(user_num+1, movie_num+1,10, [10, 16]).to(device)\nmodel_4 = torch.compile(model_4)\noptimizer = optim.Adam(model_4.parameters(), lr=0.0001, weight_decay=1e-5)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n\ncriterion = nn.BCEWithLogitsLoss()\nscaler = torch.amp.GradScaler('cuda')\n\nuser_input = torch.tensor(user_input, dtype=torch.long).to(device)\nmovie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\nlabels = torch.tensor(labels, dtype=torch.float32).to(device)\n\ndataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\n# val_user_input = []\n# val_movie_input = []\n# val_labels = []\n\n# for u in val_dict.keys():\n#     for (movie_id, label) in val_dict[u]:\n#         val_user_input.append(u)\n#         val_movie_input.append(movie_id)\n#         val_labels.append(label)\nval_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n\nval_dict = defaultdict(list)\n\nfor user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n    val_dict[user_id].append((movie_id, label))\n\nval_dict = dict(val_dict)\n\nval_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\nval_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\nval_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n\nval_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\nfor epoch in range(num_epochs):\n    model_4.train()\n    total_loss = 0\n\n    for batch_users, batch_items, batch_labels in dataloader:\n        batch_users = batch_users.to(device)\n        batch_items = batch_items.to(device)\n        batch_labels = batch_labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda'):\n            predictions = model_4(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n\n    model_4.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_users, batch_items, batch_labels in val_dataloader:\n            batch_users = batch_users.to(device)\n            batch_items = batch_items.to(device)\n            batch_labels = batch_labels.to(device)\n            predictions = model_4(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n            val_loss += loss.item()\n        val_loss_avg = val_loss / len(val_dataloader)\n        scheduler.step(val_loss_avg)\n        print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n    train_losses_rate.append(total_loss / len(dataloader))\n    val_losses_rate.append(val_loss_avg)\n        \n    _, _, f1 = model_evaluation(model, val_dict, device, K=10)\n    # train_recalls_rate.append(train_recall)\n    # train_ndcgs_rate.append(train_ndcg)\n    f1s_rate.append(f1)\n    # early stop\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        counter = 0  \n        torch.save(model_4.state_dict(), \"./best_model_rate.pth\") \n    else:\n        counter += 1\n        print(f\"Early Stopping Counter: {counter}/{patience}\")\n        if counter >= patience:\n            print(\"Early stopping triggered! Stopping training.\")\n            break   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:05:36.352453Z","iopub.execute_input":"2025-03-24T22:05:36.352759Z","iopub.status.idle":"2025-03-24T22:43:49.53607Z","shell.execute_reply.started":"2025-03-24T22:05:36.352729Z","shell.execute_reply":"2025-03-24T22:43:49.53525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses_rate3 = []\nval_losses_rate3 = []\nrecalls_rate3 = []\nndcgs_rate3 = []\nf1s_rate3 = []\ntrain_recalls_rate3 = []\ntrain_ndcgs_rate3 = []\npatience = 10\ncounter = 0\nbest_val_loss = float('inf')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/kaggle/input/mmmmmm/ratings.dat', threshold=3)\nnegative_num = 10\nnon_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n\nuser_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n# latent_dim = 8\n\nbatch_size = 256\nnum_epochs = 30\nmodel_3 = NeuMF(user_num+1, movie_num+1,10, [10, 16]).to(device)\nmodel_3 = torch.compile(model_3)\noptimizer = optim.Adam(model_3.parameters(), lr=0.0001, weight_decay=1e-5)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n\ncriterion = nn.BCEWithLogitsLoss()\nscaler = torch.amp.GradScaler('cuda')\n\nuser_input = torch.tensor(user_input, dtype=torch.long).to(device)\nmovie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\nlabels = torch.tensor(labels, dtype=torch.float32).to(device)\n\ndataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# val_user_input = []\n# val_movie_input = []\n# val_labels = []\n\n# for u in val_dict.keys():\n#     for (movie_id, label) in val_dict[u]:\n#         val_user_input.append(u)\n#         val_movie_input.append(movie_id)\n#         val_labels.append(label)\nval_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\nval_dict = defaultdict(list)\n\nfor user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n    val_dict[user_id].append((movie_id, label))\n\nval_dict = dict(val_dict)\n\nval_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\nval_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\nval_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n\nval_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n\nfor epoch in range(num_epochs):\n    model_3.train()\n    total_loss = 0\n\n    for batch_users, batch_items, batch_labels in dataloader:\n        batch_users = batch_users.to(device)\n        batch_items = batch_items.to(device)\n        batch_labels = batch_labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda'):\n            predictions = model_3(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n\n    model_3.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_users, batch_items, batch_labels in val_dataloader:\n            batch_users = batch_users.to(device)\n            batch_items = batch_items.to(device)\n            batch_labels = batch_labels.to(device)\n            predictions = model_3(batch_users, batch_items)\n            loss = criterion(predictions, batch_labels.view(-1, 1))\n            val_loss += loss.item()\n        val_loss_avg = val_loss / len(val_dataloader)\n        scheduler.step(val_loss_avg)\n        print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n    train_losses_rate3.append(total_loss / len(dataloader))\n    val_losses_rate3.append(val_loss_avg)\n        \n    _, _, f1 = model_evaluation(model, val_dict, device, K=10)\n    # train_recall, train_ndcg = model_evaluation(model_3, train_dict, device, K=10)\n    # train_recalls_rate3.append(train_recall)\n    # train_ndcgs_rate3.append(train_ndcg)\n    f1s_rate3.append(f1)\n    # early stop\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        counter = 0  \n        torch.save(model_3.state_dict(), \"./best_model_rate3.pth\") \n    else:\n        counter += 1\n        print(f\"Early Stopping Counter: {counter}/{patience}\")\n        if counter >= patience:\n            print(\"Early stopping triggered! Stopping training.\")\n            break   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:43:49.536942Z","iopub.execute_input":"2025-03-24T22:43:49.537167Z","iopub.status.idle":"2025-03-25T00:39:49.127241Z","shell.execute_reply.started":"2025-03-24T22:43:49.537149Z","shell.execute_reply":"2025-03-25T00:39:49.126309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nepochs = np.arange(1, num_epochs + 1)\n\ntrain_losses = np.concatenate([train_losses, [np.nan] * (len(epochs) - len(train_losses))]) if len(train_losses) < len(epochs) else train_losses\nval_losses = np.concatenate([val_losses, [np.nan] * (len(epochs) - len(val_losses))]) if len(val_losses) < len(epochs) else val_losses\n# train_losses_rate = np.concatenate([train_losses_rate, [np.nan] * (len(epochs) - len(train_losses_rate))]) if len(train_losses_rate) < len(epochs) else train_losses_rate\n# val_losses_rate = np.concatenate([val_losses_rate, [np.nan] * (len(epochs) - len(val_losses_rate))]) if len(val_losses_rate) < len(epochs) else val_losses_rate\n# train_losses_meanStd = np.concatenate([train_losses_meanStd, [np.nan] * (len(epochs) - len(train_losses_meanStd))]) if len(train_losses_meanStd) < len(epochs) else train_losses_meanStd\n# val_losses_meanStd = np.concatenate([val_losses_meanStd, [np.nan] * (len(epochs) - len(val_losses_meanStd))]) if len(val_losses_meanStd) < len(epochs) else val_losses_meanStd\n# train_losses_rate3 = np.concatenate([train_losses_rate3, [np.nan] * (len(epochs) - len(train_losses_rate3))]) if len(train_losses_rate3) < len(epochs) else train_losses_rate3\n# val_losses_rate3 = np.concatenate([val_losses_rate3, [np.nan] * (len(epochs) - len(val_losses_rate3))]) if len(val_losses_rate3) < len(epochs) else val_losses_rate3\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(5, 15), sharex=True)\n\nax1.plot(epochs, train_losses, label='Training Loss (Filter 3)', linestyle='-', color='#1f77b4', linewidth=2)\nax1.plot(epochs, val_losses, label='Validation Loss (Filter 3)', linestyle='-', color='#ff7f0e', linewidth=2)\nax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\nax1.legend(loc='upper right', fontsize=8, frameon=True)\nax1.grid(True, linestyle='--', alpha=0.5)\nax1.set_title('Training & Validation Loss (Filter 3)', fontsize=14, fontweight='bold')\n\n# ax2.plot(epochs, train_losses_rate, label='Training Loss (≥4)', linestyle='--', color='red', linewidth=2)\n# ax2.plot(epochs, val_losses_rate, label='Validation Loss (≥4)', linestyle='-.', color='green', linewidth=2)\n# ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n# ax2.set_ylabel('Loss Rate', fontsize=12, fontweight='bold')\n# ax2.legend(loc='upper right', fontsize=8, frameon=True)\n# ax2.grid(True, linestyle='--', alpha=0.5)\n# ax2.set_title('Training & Validation Loss (Threshold ≥ 4)', fontsize=14, fontweight='bold')\n\n# ax3.plot(epochs, train_losses_meanStd, label='Training Loss (Std Mean)', linestyle='--', color='red', linewidth=2)\n# ax3.plot(epochs, val_losses_meanStd, label='Validation Loss (Std Mean)', linestyle='-.', color='green', linewidth=2)\n# ax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n# ax3.set_ylabel('Loss Rate', fontsize=12, fontweight='bold')\n# ax3.legend(loc='upper right', fontsize=8, frameon=True)\n# ax3.grid(True, linestyle='--', alpha=0.5)\n# ax3.set_title('Training & Validation Loss (Standardized Mean)', fontsize=14, fontweight='bold')\n\n# ax4.plot(epochs, train_losses_rate3, label='Training Loss (≥3)', linestyle='--', color='red', linewidth=2)\n# ax4.plot(epochs, val_losses_rate3, label='Validation Loss (≥3)', linestyle='-.', color='green', linewidth=2)\n# ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n# ax4.set_ylabel('Loss', fontsize=12, fontweight='bold')\n# ax4.legend(loc='upper right', fontsize=11, frameon=True)\n# ax4.grid(True, linestyle='--', alpha=0.5)\n# ax4.set_title('Training & Validation Loss (Threshold ≥ 3)', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"id":"lFwSdg1g9nmB","execution":{"iopub.status.busy":"2025-03-25T10:04:34.722789Z","iopub.execute_input":"2025-03-25T10:04:34.723101Z","iopub.status.idle":"2025-03-25T10:04:35.597747Z","shell.execute_reply.started":"2025-03-25T10:04:34.723073Z","shell.execute_reply":"2025-03-25T10:04:35.59689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nepochs = np.arange(1, num_epochs + 1)\n\n# Ensure all arrays have the same length as epochs\nf1s = np.concatenate([f1s, [np.nan] * (len(epochs) - len(f1s))]) if len(f1s) < len(epochs) else f1s\n# f1s_rate = np.concatenate([f1s_rate, [np.nan] * (len(epochs) - len(f1s_rate))]) if len(f1s_rate) < len(epochs) else f1s_rate\n# f1s_meanStd = np.concatenate([f1s_meanStd, [np.nan] * (len(epochs) - len(f1s_meanStd))]) if len(f1s_meanStd) < len(epochs) else f1s_meanStd\n# f1s_rate3 = np.concatenate([f1s_rate3, [np.nan] * (len(epochs) - len(f1s_rate3))]) if len(f1s_rate3) < len(epochs) else f1s_rate3\n\n# Create a single plot\nplt.figure(figsize=(5, 5))\n\nplt.plot(epochs, f1s, label='F1 (Filter 3)', marker='o', linestyle='--', color='red', linewidth=2)\n# plt.plot(epochs, f1s_rate, label='F1 (≥4)', marker='s', linestyle='-', color='blue', linewidth=2)\n# plt.plot(epochs, f1s_meanStd, label='F1 (StdMean)', marker='^', linestyle='-.', color='green', linewidth=2)\n# plt.plot(epochs, f1s_rate3, label='F1 (≥3)', marker='d', linestyle=':', color='purple', linewidth=2)\n\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('F1', fontsize=12, fontweight='bold')\nplt.legend(loc='center right', fontsize=11, frameon=True)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.title('F1 Comparison', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:43:59.548037Z","iopub.execute_input":"2025-03-25T10:43:59.548414Z","iopub.status.idle":"2025-03-25T10:43:59.622303Z","shell.execute_reply.started":"2025-03-25T10:43:59.548383Z","shell.execute_reply":"2025-03-25T10:43:59.621146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# epochs = np.arange(1, num_epochs + 1)\n\n# # Ensure all arrays have the same length as epochs\n# recalls = np.concatenate([recalls, [np.nan] * (len(epochs) - len(recalls))]) if len(recalls) < len(epochs) else recalls\n# recalls_rate = np.concatenate([recalls_rate, [np.nan] * (len(epochs) - len(recalls_rate))]) if len(recalls_rate) < len(epochs) else recalls_rate\n# recalls_meanStd = np.concatenate([recalls_meanStd, [np.nan] * (len(epochs) - len(recalls_meanStd))]) if len(recalls_meanStd) < len(epochs) else recalls_meanStd\n# recalls_rate3 = np.concatenate([recalls_rate3, [np.nan] * (len(epochs) - len(recalls_rate3))]) if len(recalls_rate3) < len(epochs) else recalls_rate3\n\n# # Create a single plot\n# plt.figure(figsize=(5, 5))\n\n# plt.plot(epochs, recalls, label='Recall (Filter 3)', marker='o', linestyle='--', color='red', linewidth=2)\n# plt.plot(epochs, recalls_rate, label='Recall (≥4)', marker='s', linestyle='-', color='blue', linewidth=2)\n# plt.plot(epochs, recalls_meanStd, label='Recall (StdMean)', marker='^', linestyle='-.', color='green', linewidth=2)\n# plt.plot(epochs, recalls_rate3, label='Recall (≥3)', marker='d', linestyle=':', color='purple', linewidth=2)\n\n# plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n# plt.ylabel('Recall', fontsize=12, fontweight='bold')\n# plt.legend(loc='center right', fontsize=11, frameon=True)\n# plt.grid(True, linestyle='--', alpha=0.5)\n# plt.title('Recall Comparison Across Different Rate Setting', fontsize=14, fontweight='bold')\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:39:50.061748Z","iopub.status.idle":"2025-03-25T00:39:50.062147Z","shell.execute_reply":"2025-03-25T00:39:50.061977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntest_user_input, test_movie_input, test_labels = get_input_data_nointeract(test_dict, non_interacted_movies)\ntest_dict = defaultdict(list)\n\nfor user_id, movie_id, label in zip(test_user_input, test_movie_input, test_labels):\n    test_dict[user_id].append((movie_id, label))\n\ntest_dict = dict(test_dict)\n_, _, f1 = model_evaluation(model, test_dict, device, K=10)\n# _, _, f1_meanStd = model_evaluation(model_meanStd, test_dict, device, K=10)\n# _, _, f1_4 = model_evaluation(model_4, test_dict, device, K=10)\n# _, _, f1_3 = model_evaluation(model_3, test_dict, device, K=10)\n\nmodels = [' Filter3', 'MeanStd', 'rate ≥4', 'rate ≥3']\nf1_scores = [f1]\n\nx = np.arange(len(models))  # x-axis positions\nwidth = 0.4  # Bar width\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot Recall comparison\nax1.bar(x, f1_scores, width, color=['blue', 'orange', 'green', 'red'])\nax1.set_ylabel('F1', fontsize=12, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(models, fontsize=11)\nax1.set_title('F1 Comparison', fontsize=14, fontweight='bold')\nax1.grid(axis='y', linestyle='--', alpha=0.6)\n\n# # Plot NDCG comparison\n# ax2.bar(x, ndcg_scores, width, color=['blue', 'orange', 'green', 'red'])\n# ax2.set_ylabel('NDCG@10', fontsize=12, fontweight='bold')\n# ax2.set_xticks(x)\n# ax2.set_xticklabels(models, fontsize=11)\n# ax2.set_title('NDCG Comparison', fontsize=14, fontweight='bold')\n# ax2.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-25T10:06:04.106061Z","iopub.execute_input":"2025-03-25T10:06:04.10644Z","execution_failed":"2025-03-25T10:43:36.459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(recall_scores)\n# print(ndcg_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T00:39:50.064223Z","iopub.status.idle":"2025-03-25T00:39:50.0646Z","shell.execute_reply":"2025-03-25T00:39:50.064434Z"}},"outputs":[],"execution_count":null}]}